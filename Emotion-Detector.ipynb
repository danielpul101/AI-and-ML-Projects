{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT8zOTTJOUt-"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# AI Emotions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following cells to import the required modules and libraries. You will also clone into the yolov5 repository from ultraytics."
      ],
      "metadata": {
        "id": "9O8Vpz3cwDA2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoJ4hGiBOUuI"
      },
      "outputs": [],
      "source": [
        "#https://pytorch.org/get-started/locally/\n",
        "#!pip3 install torch torchvision torchaudio\n",
        "#!git clone https://github.com/Emilien-mipt/fer-pytorch\n",
        "#!pip install fer-pytorch\n",
        "#!pip install -r requirements/requirements-dev.txt\n",
        "!pip install PyEmotion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PyEmotion import *\n",
        ">>> PyEmotion()"
      ],
      "metadata": {
        "id": "VNcdSKf17E37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "\n",
        "from PyEmotion import DetectFace\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to capture a photo using JavaScript\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "    js = Javascript('''\n",
        "        async function takePhoto(quality) {\n",
        "            const div = document.createElement('div');\n",
        "            const capture = document.createElement('button');\n",
        "            capture.textContent = 'Capture';\n",
        "            div.appendChild(capture);\n",
        "\n",
        "            const video = document.createElement('video');\n",
        "            video.style.display = 'block';\n",
        "            const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "            document.body.appendChild(div);\n",
        "            div.appendChild(video);\n",
        "            video.srcObject = stream;\n",
        "            await video.play();\n",
        "\n",
        "            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "            await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "            const canvas = document.createElement('canvas');\n",
        "            canvas.width = video.videoWidth;\n",
        "            canvas.height = video.videoHeight;\n",
        "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "            stream.getVideoTracks()[0].stop();\n",
        "            div.remove();\n",
        "            return canvas.toDataURL('image/jpeg', quality);\n",
        "        }\n",
        "    ''')\n",
        "    display(js)\n",
        "    data = eval_js('takePhoto({})'.format(quality))\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(binary)\n",
        "    return filename\n",
        "\n",
        "# Initialize PyEmotion Detector\n",
        "er = DetectFace(device='gpu', gpu_id=0)  # Using GPU\n",
        "\n",
        "# Function to process and display emotion detection\n",
        "def process_and_show_emotion():\n",
        "    photo = take_photo()  # Capture a photo\n",
        "    frame = cv.imread(photo)  # Read the captured image\n",
        "    frame, emotion = er.predict_emotion(frame)  # Predict emotion\n",
        "    plt.imshow(cv.cvtColor(frame, cv.COLOR_BGR2RGB))  # Convert color to RGB for matplotlib\n",
        "    plt.title(\"Emotion: \" + emotion)\n",
        "    plt.show()  # Display the image with the detected emotion\n",
        "\n",
        "# Call the function to capture an image and detect emotion\n",
        "process_and_show_emotion()\n"
      ],
      "metadata": {
        "id": "Yf_uJcQq7lFh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "yolo",
      "language": "python",
      "name": "yolo"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vasHQinQOUuJ",
        "DLLEayhWOUuK",
        "hvsvCPCkOUuK",
        "qG7yA5tslQnU",
        "rkS8OioVZvuO",
        "skZAyCZJwDU3",
        "tHlJF3_aC7O9",
        "JQ85yAuccHdu"
      ],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}